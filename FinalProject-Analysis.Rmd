---
title: "Final Project Analysis"
author: "Matt McCoy, HwiJoon Lee, Tanvi Magdum"
date: "23 April 2024"
output: html_notebook
---

```{r InstallPackages, results='hide', echo=FALSE}
if(!require(dplyr)) install.packages("dplyr")
if(!require(gginference)) install.packages("gginference")
library("ggpubr")
```

### 

```{r, results='hide', echo=FALSE}
survey <- read.csv("dataset/CleanedSurvey.csv", stringsAsFactors = TRUE)
survey
```

### Did the grade in OOD change depending on the AI usage?

**Null Hypothesis:** There is no difference in mean grades between students who used AI and those who did not.

**Alternate Hypothesis:** There is a significant difference in mean grades between the students who used AI and those who did not.

**Wilcoxon Rank-Sum Test**

```{r}
# Create a categorical variable for AI usage
survey$AI_Usage <- ifelse(survey$How.much.have.you.used...are.you.using.AI.in.OOD.if.at.all.. > 0, "AI User", "Non-AI User")

# 1. Normality Assumption

# Shapiro-Wilk Test
shapiro_ai <- shapiro.test(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "AI User"])
shapiro_non_ai <- shapiro.test(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "Non-AI User"])

# Print Shapiro-Wilk test results
print(shapiro_ai)
print(shapiro_non_ai)

# QQ plots for pss and loneliness scores
qqnorm(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "AI User"],
       main = "AI User Grade Distribution")
qqnorm(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "Non-AI User"],
       main = "Non-AI User Grade Distribution")
```

The normality assumption fails for AI user group since the p-value is less than significance level. Also, in QQ plot the datapoints do not follow a straight line. Hence, grades data is not normally distributed for AI-users.

Let's calculate variances and check if any outliers are present.

```{r}
# Calculate variances
var_ai <- var(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "AI User"])
var_non_ai <- var(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "Non-AI User"])

# Print the variances
print(paste("Variance of grades for AI users:", var_ai))
print(paste("Variance of grades for Non-AI users:", var_non_ai))

# Calculate IQR for AI users
Q1_ai <- quantile(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "AI User"], 0.25)
Q3_ai <- quantile(survey$What.was.is.your.grade.from.OOD...0.100.[survey$AI_Usage == "AI User"], 0.75)
IQR_ai <- Q3_ai - Q1_ai

# Calculate IQR for non-AI users
Q1_non_ai <- quantile(survey$What.was.or.is.your.grade.from.OOD.[survey$AI_Usage == "Non-AI User"], 0.25)
Q3_non_ai <- quantile(survey$What.was.or.is.your.grade.from.OOD.[survey$AI_Usage == "Non-AI User"], 0.75)
IQR_non_ai <- Q3_non_ai - Q1_non_ai


# Define thresholds for outliers
lower_threshold_ai <- Q1_ai - 1.5 * IQR_ai
upper_threshold_ai <- Q3_ai + 1.5 * IQR_ai

lower_threshold_non_ai <- Q1_non_ai - 1.5 * IQR_non_ai
upper_threshold_non_ai <- Q3_non_ai + 1.5 * IQR_non_ai


# Identify outliers
outliers_ai <- survey$What.was.or.is.your.grade.from.OOD.[survey$AI_Usage == "AI User" & (survey$What.was.or.is.your.grade.from.OOD. < lower_threshold_ai | survey$What.was.or.is.your.grade.from.OOD. > upper_threshold_ai)]

outliers_non_ai <- survey$What.was.or.is.your.grade.from.OOD.[survey$AI_Usage == "Non-AI User" & (survey$What.was.or.is.your.grade.from.OOD. < lower_threshold_non_ai | survey$What.was.or.is.your.grade.from.OOD. > upper_threshold_non_ai)]


# Print outliers
print("Outliers for AI users:")
print(outliers_ai)

print("Outliers for non-AI users:")
print(outliers_non_ai)

# Boxplot for identifying outliers
boxplot(What.was.is.your.grade.from.OOD...0.100. ~ AI_Usage, data=survey, 
        xlab="AI Usage", ylab="Grades", main="Boxplot of Grades by AI Usage")
```

***Test choice:***

Given the violation of normality assumptions for the grades of AI users, a non-parametric test such as the Wilcoxon rank-sum test (Mann-Whitney U test) would be more appropriate. Wilcoxon rank-sum test will be used for comparing the grades of AI users and Non-AI users, as it is robust to violations of normality assumptions and the presence of outliers.

```{r}
# Perform wilcoxon rank-sum test
wilcox_test_1 <- wilcox.test(What.was.is.your.grade.from.OOD...0.100. ~ AI_Usage, data = survey, alternative = "two.sided", exact=FALSE)

# Print the results of the Wilcoxon rank-sum test
print(wilcox_test_1)
```

***Analysis:***

Since the p-value (0.7943) is greater than 0.05, we fail to reject the null hypothesis. This means that there is not enough evidence to prove that there is a significant difference in grades between the students who used AI and those who did not in OOD.

### Does the use of code generation impact the academic performance in OOD?

**Null Hypothesis:** The use of code generation does not affect the academic performance in OOD.

**Alternate Hypothesis:** The use of code generation affect the academic performance in OOD.

**Statistical Test: T-Test / Trying to compare means between two groups.**

```{r}
survey$CodeGeneration <- ifelse(survey$How.do.you.use.AI.to.assist.your.work.if.you.use.them. == "Code Generation", "Yes", "No")

survey$What.was.is.your.grade.from.OOD. <- as.numeric(as.character(survey$What.was.is.your.grade.from.OOD.))

grades_codeGen <- survey[survey$CodeGeneration == "Yes", "What.was.is.your.grade.from.OOD."]
grades_noCodeGen <- survey[survey$CodeGeneration == "No", "What.was.is.your.grade.from.OOD."]

my_data <- survey[c("CodeGeneration", "What.was.is.your.grade.from.OOD.")]
names(my_data) <- c("group", "grades")

ggboxplot(my_data, x = "group", y = "grades",
          color = "group", palette = c("#00AFBB", "#E7B800"),
          order = c("Yes", "No"),
          ylab = "Grades from OOD", xlab = "Use of AI for Code Generation")

shapiro.test(grades_codeGen)
shapiro.test(grades_noCodeGen)

#removing outlier
grades_codeGen_no_outlier <- grades_codeGen[grades_codeGen != 68]

grades_noCodeGen_no_outlier <- grades_noCodeGen[grades_noCodeGen != 76]

```

```{r}
t.test(grades_codeGen, grades_noCodeGen, var.equal = FALSE)
t.test(grades_codeGen_no_outlier, grades_noCodeGen, var.equal = FALSE)

```

### Impact of Subscription-based model

```{r}
# categorize it into true of it has subscription as response
survey$IsSubscriptionModel <- grepl("subscription", survey$What.kind.of.AI.Model.did.you.use, ignore.case = TRUE)
# checks if they used AI because I am comparing if Sub model vs free model
survey$UsedAI <- !grepl("Did Not Use", survey$What.kind.of.AI.Model.did.you.use, ignore.case = TRUE)

subscription_model_grades <- survey[survey$IsSubscriptionModel == TRUE, "What.was.is.your.grade.from.OOD."]
no_subscription_model_grades <- survey[survey$IsSubscriptionModel == FALSE & survey$UsedAI == TRUE, "What.was.is.your.grade.from.OOD."]



shapiro.test(no_subscription_model_grades)

```

```{r}
grades_data <- data.frame(
  grades = c(subscription_model_grades, no_subscription_model_grades),
  group = c(rep("Subscription", length(subscription_model_grades)), rep("Free Model", length(no_subscription_model_grades)))
)

# Generate the boxplot
ggboxplot(grades_data, x = "group", y = "grades",
          color = "group", palette = c("#00AFBB", "#E7B800"),
          order = c("Subscription", "Free Model"),
          ylab = "Grades from OOD", xlab = "Type of AI Model Used") +
  theme_minimal()
```

```{r}
ggboxplot(my_data, x = "group", y = "grades",
          color = "group", palette = c("#00AFBB", "#E7B800"),
          order = c("Yes", "No"),
          ylab = "Grades from OOD", xlab = "Use of AI for Code Generation")

t.test(subscription_model_grades, no_subscription_model_grades, var.equal = FALSE)

```

### Does the use of code generation impact the academic performance in OOD?

**Null Hypothesis:** The use of code generation does not affect the academic performance in OOD.

**Alternate Hypothesis:** The use of code generation affect the academic performance in OOD.

**Statistical Test: T-Test / Trying to compare means between two groups.**

### Does AI effect how much time students spent on Object Oriented Design per week?

**Null Hypothesis:** AI does not effect how much time students spend on Object Oriented Design per week.

**Alternate Hypothesis:** AI does effect how much time students spend on Object Oriented Design per week.

**Statistical Test: Wilcoxon**

***Reasoning:*** Conducting a Wilcoxon rank-sum test for this question will produce the best result as comparing the mean number of hours of people who utilize AI in OOD versus those who do not and the data given is not normal as seen from the shapiro tests below. Will answer whether or not AI had an effect on overall time spent per week doing assignments for the course.

```{r}
noAiOnHoursSpent <- survey %>% filter(grepl("Did not use", What.kind.of.AI.Model.did.you.use.))
AiOnHoursSpent <- survey %>% filter(!grepl("Did not use", What.kind.of.AI.Model.did.you.use.))

newDF = data.frame("hours" = noAiOnHoursSpent$How.many.hours.did.you.spend.on.OOD.per.week., "ai" = "Did not use")
combinedDF = rbind(newDF, data.frame("hours" = AiOnHoursSpent$How.many.hours.did.you.spend.on.OOD.per.week., "ai" = "used"))


boxplot(combinedDF$hours ~ combinedDF$ai, ylab = "Hours Spent on Course", xlab = "Did they use AI in the course?")

shapiro.test(noAiOnHoursSpent$How.many.hours.did.you.spend.on.OOD.per.week.)
shapiro.test(AiOnHoursSpent$How.many.hours.did.you.spend.on.OOD.per.week.)

wilcox.test(noAiOnHoursSpent$How.many.hours.did.you.spend.on.OOD.per.week., AiOnHoursSpent$How.many.hours.did.you.spend.on.OOD.per.week., alternative = "greater")
```

***Analysis:***

Since the p-value (0.2241) is greater than 0.05, we fail to reject the null hypothesis. This means that there isnt enough evidence to say that there is a significant effect on time spent per week in OOD if someone is utilizing AI verses someone who is not.

In conclusion based on the data and the results of the test, we do not have sufficient evidence to conclude that there is a significant effect in time spent per week in Object Oriented Design if you are using AI.

**Determining Outliers**

As seen in the above plots there are no visible outliers. So we do not need to do anything further.

### Does the AI usage frequency affect the work hours per week?

**Null Hypothesis:** AI usage frequency does not affect how much time AI-users spend on OOD per week.

**Alternate Hypothesis:** AI usage frequency significantly affects how much time AI-users spend on OOD per week.

**Statistical Test: One-way ANOVA.**

```{r}
# Exclude non-AI users
data <- survey[survey$AI_Usage == "AI User",]
# Divide AI usage frequency into categories
data$AI_usage_category <- ifelse(data$How.much.have.you.used...are.you.using.AI.in.OOD.if.at.all.. <= 6, "Low", "High")

head(data)

```

```{r}
# 1. Normality Assumption

# Shapiro-Wilk Test
shapiro_test_low <- shapiro.test(data[data$AI_usage_category == "Low", ]$`How.many.hours.did.you.spend.on.OOD.per.week.`)
shapiro_test_high <- shapiro.test(data[data$AI_usage_category == "High", ]$`How.many.hours.did.you.spend.on.OOD.per.week.`)

# Print the results
print(shapiro_test_low)
print(shapiro_test_high)

# QQ plots for each AI usage category
qqnorm(data[data$AI_usage_category == "Low", ]$`How.many.hours.did.you.spend.on.OOD.per.week.`,
       main = "Low AI-User Spent Time Distribution")
qqline(data[data$AI_usage_category == "Low", ]$`How.many.hours.did.you.spend.on.OOD.per.week.`)

qqnorm(data[data$AI_usage_category == "High", ]$`How.many.hours.did.you.spend.on.OOD.per.week.`,
       main = "High AI-User Spent Time Distribution")
qqline(data[data$AI_usage_category == "High", ]$`How.many.hours.did.you.spend.on.OOD.per.week.`)

```

Based on the Shapiro-Wilk normality tests and the non-straight QQ plots, the assumption of normality is violated for both the "Low" and "High" AI usage frequency categories.

Let us check outliers.

```{r}
# Calculate IQR for Low AI users
Q1_low_ai <- quantile(data$How.many.hours.did.you.spend.on.OOD.per.week.[data$AI_usage_category == "Low"], 0.25)
Q3_low_ai <- quantile(data$How.many.hours.did.you.spend.on.OOD.per.week.[data$AI_usage_category == "Low"], 0.75)
IQR_low_ai <- Q3_low_ai - Q1_low_ai

# Calculate IQR for High AI users
Q1_high_ai <- quantile(data$How.many.hours.did.you.spend.on.OOD.per.week.[data$AI_usage_category == "High"], 0.25)
Q3_high_ai <- quantile(data$How.many.hours.did.you.spend.on.OOD.per.week.[data$AI_usage_category == "High"], 0.75)
IQR_high_ai <- Q3_high_ai - Q1_high_ai


# Define thresholds for outliers
lower_threshold_low_ai <- Q1_low_ai - 1.5 * IQR_low_ai
upper_threshold_low_ai <- Q3_low_ai + 1.5 * IQR_low_ai

lower_threshold_high_ai <- Q1_high_ai - 1.5 * IQR_high_ai
upper_threshold_high_ai <- Q3_high_ai + 1.5 * IQR_high_ai


# Identify outliers
outliers_low_ai <- data$How.many.hours.did.you.spend.on.OOD.per.week.[data$AI_usage_category == "Low" & (data$How.many.hours.did.you.spend.on.OOD.per.week. < lower_threshold_low_ai | data$How.many.hours.did.you.spend.on.OOD.per.week. > upper_threshold_low_ai)]

outliers_high_ai <- data$How.many.hours.did.you.spend.on.OOD.per.week.[data$AI_usage_category == "High" & (data$How.many.hours.did.you.spend.on.OOD.per.week. < lower_threshold_high_ai | data$How.many.hours.did.you.spend.on.OOD.per.week. > upper_threshold_high_ai)]


# Print outliers
print("Outliers for Low AI users:")
print(outliers_low_ai)

print("Outliers for High AI users:")
print(outliers_high_ai)

# Boxplot for identifying outliers
boxplot(How.many.hours.did.you.spend.on.OOD.per.week. ~ AI_usage_category, data=data, 
        xlab="AI Usage Category", ylab="Time Spent on OOD", main="Boxplot of Time Spent on OOD by AI Usage Category")
```

Though we see some outliers in Low AI-users category, we cannot remove them, as the frequency of same outliers is more, meaning the data could be legitimate.

***Test choice:***

The choice of the Wilcoxon rank sum test is appropriate due to the violation of the normality assumption observed in the Shapiro-Wilk tests and the non-straight QQ plots. Also, there are few outliers which cannot be excluded, so Wilcoxon is the best choice as it is robust to normality and outliers both.

```{r}
# Perform Wilcoxon rank sum test
wilcox_test_3.2 <- wilcox.test(How.many.hours.did.you.spend.on.OOD.per.week. ~ AI_usage_category, data = data, alternative = "two.sided", exact=FALSE)

# Print the results of the Wilcoxon rank sum test
print(wilcox_test_3.2)
```

Since the p-value (0.6221) is greater than 0.05, we fail to reject the null hypothesis. Therefore, there is not enough evidence to conclude that there is a significant difference in the work hours per week between the "Low" and "High" AI usage frequency categories.
